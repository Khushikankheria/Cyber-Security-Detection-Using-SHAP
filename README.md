## Cyber Security Detection Using XAI

This project demonstrates how Explainable Artificial Intelligence (XAI) can be applied to cybersecurity threat detection. Traditional ML models can identify threats but often work like a black box. By integrating XAI methods such as SHAP, LIME, and feature importance, this project provides both high accuracy in detecting threats and interpretability, helping security analysts understand why a model flagged a threat.

#Features

ğŸ“Š Threat Detection using ML classifiers.

ğŸ” Explainability with XAI â€“ SHAP, LIME, and feature importance visualizations.

âš¡ Efficient Preprocessing â€“ handles categorical, numerical, and imbalanced data.

ğŸ“ˆ Model Evaluation â€“ accuracy, precision, recall, F1-score, ROC curves.

ğŸ¨ Visual Explanations â€“ feature impact and per-sample explanations.

ğŸ”’ Cybersecurity-focused â€“ phishing and network threat datasets.

#Tech Stack

Language: Python ğŸ
Libraries:
pandas, numpy â†’ data preprocessing
scikit-learn â†’ ML models & metrics
xgboost â†’ gradient boosting classifier
matplotlib, seaborn â†’ visualizations
shap â†’ explainability
Environment: Jupyter Notebook

#Results

Achieved high classification accuracy across datasets using ensemble models.

Random Forest / XGBoost performed best in detection.

XAI visualizations showed that:

For phishing, URL-based features (e.g., length, presence of @, redirects) were most important.

For network threats, protocol type and connection duration were key indicators.

Final output included interpretable dashboards of feature contributions.
